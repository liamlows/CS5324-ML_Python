{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Recurrent Neural Networks\n",
    "##### By. Liam Lowsley-Williams & Fernando Vazquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.0.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# graph visualization\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewTitle</th>\n",
       "      <th>ReviewBody</th>\n",
       "      <th>ReviewStar</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honest review of an edm music lover\\n</td>\n",
       "      <td>No doubt it has a great bass and to a great ex...</td>\n",
       "      <td>3</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unreliable earphones with high cost\\n</td>\n",
       "      <td>This  earphones are unreliable, i bought it be...</td>\n",
       "      <td>1</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good and durable.\\n</td>\n",
       "      <td>i bought itfor 999,I purchased it second time,...</td>\n",
       "      <td>4</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped working in just 14 days\\n</td>\n",
       "      <td>Its sound quality is adorable. overall it was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just Awesome Wireless Headphone under 1000...ðŸ˜‰\\n</td>\n",
       "      <td>Its Awesome... Good sound quality &amp; 8-9 hrs ba...</td>\n",
       "      <td>5</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14332</th>\n",
       "      <td>Good\\n</td>\n",
       "      <td>Good\\n</td>\n",
       "      <td>4</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14333</th>\n",
       "      <td>Amazing Product\\n</td>\n",
       "      <td>An amazing product but a bit costly.\\n</td>\n",
       "      <td>5</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14334</th>\n",
       "      <td>Not bad\\n</td>\n",
       "      <td>Sound\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14335</th>\n",
       "      <td>a good product\\n</td>\n",
       "      <td>the sound is good battery life is good but the...</td>\n",
       "      <td>5</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14336</th>\n",
       "      <td>Average headphones , n overrated name\\n</td>\n",
       "      <td>M writing this review after using for almost 7...</td>\n",
       "      <td>1</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14337 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ReviewTitle  \\\n",
       "0                 Honest review of an edm music lover\\n   \n",
       "1                 Unreliable earphones with high cost\\n   \n",
       "2                            Really good and durable.\\n   \n",
       "3                     stopped working in just 14 days\\n   \n",
       "4      Just Awesome Wireless Headphone under 1000...ðŸ˜‰\\n   \n",
       "...                                                 ...   \n",
       "14332                                            Good\\n   \n",
       "14333                                 Amazing Product\\n   \n",
       "14334                                         Not bad\\n   \n",
       "14335                                  a good product\\n   \n",
       "14336           Average headphones , n overrated name\\n   \n",
       "\n",
       "                                              ReviewBody  ReviewStar  \\\n",
       "0      No doubt it has a great bass and to a great ex...           3   \n",
       "1      This  earphones are unreliable, i bought it be...           1   \n",
       "2      i bought itfor 999,I purchased it second time,...           4   \n",
       "3      Its sound quality is adorable. overall it was ...           1   \n",
       "4      Its Awesome... Good sound quality & 8-9 hrs ba...           5   \n",
       "...                                                  ...         ...   \n",
       "14332                                             Good\\n           4   \n",
       "14333             An amazing product but a bit costly.\\n           5   \n",
       "14334                                            Sound\\n           1   \n",
       "14335  the sound is good battery life is good but the...           5   \n",
       "14336  M writing this review after using for almost 7...           1   \n",
       "\n",
       "                Product  \n",
       "0      boAt Rockerz 255  \n",
       "1      boAt Rockerz 255  \n",
       "2      boAt Rockerz 255  \n",
       "3      boAt Rockerz 255  \n",
       "4      boAt Rockerz 255  \n",
       "...                 ...  \n",
       "14332        JBL T110BT  \n",
       "14333        JBL T110BT  \n",
       "14334        JBL T110BT  \n",
       "14335        JBL T110BT  \n",
       "14336        JBL T110BT  \n",
       "\n",
       "[14337 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"reviews.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_title = data[\"ReviewTitle\"]\n",
    "data_body = data[\"ReviewBody\"]\n",
    "y = data[\"ReviewStar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Honest review of an edm music lover\\n No doubt...\n",
       "1        Unreliable earphones with high cost\\n This  ea...\n",
       "2        Really good and durable.\\n i bought itfor 999,...\n",
       "3        stopped working in just 14 days\\n Its sound qu...\n",
       "4        Just Awesome Wireless Headphone under 1000...ðŸ˜‰...\n",
       "                               ...                        \n",
       "14332                                        Good\\n Good\\n\n",
       "14333    Amazing Product\\n An amazing product but a bit...\n",
       "14334                                    Not bad\\n Sound\\n\n",
       "14335    a good product\\n the sound is good battery lif...\n",
       "14336    Average headphones , n overrated name\\n M writ...\n",
       "Length: 14337, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = data[\"ReviewTitle\"].map(str) + \" \" + data[\"ReviewBody\"]\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5074"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12156 unique tokens. Distilled to 12156 top words.\n",
      "Shape of data tensor: (14337, 868)\n",
      "Shape of label tensor: (14337, 5)\n",
      "12156\n",
      "Wall time: 630 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_TOP_WORDS = None\n",
    "MAX_ART_LEN = X_data.map(lambda x: len(x.split())).max() # maximum and minimum number of words\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "sequences = tokenizer.texts_to_sequences(X_data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "# X = pad_sequences(sequences)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(y)\n",
    "y_ohe = y_ohe[:,1:]\n",
    "\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  149,   37,  133],\n",
       "       [   0,    0,    0, ..., 1634,    9,  301],\n",
       "       [   0,    0,    0, ...,  150,   31,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   11,   70,    6],\n",
       "       [   0,    0,    0, ...,   18,    3,    9],\n",
       "       [   0,    0,    0, ...,   82,  298,  208]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11469, 868) (11469, 5)\n",
      "(2868, 868) (2868, 5)\n",
      "[1994.  751. 1203. 2551. 4970.]\n",
      "[ 499.  188.  300.  638. 1243.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                              stratify=y, \n",
    "                                                              random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "print(np.sum(y_train,axis=0))\n",
    "print(np.sum(y_test,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 868, 150)          130200    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 155,805\n",
      "Trainable params: 155,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "max_review_length = MAX_ART_LEN\n",
    "EMBED_SIZE = 150\n",
    "embedding_layer = Embedding(X_train.shape[1],\n",
    "                            EMBED_SIZE,\n",
    "                            input_length=MAX_ART_LEN)\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(SimpleRNN(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liaml\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11469 samples, validate on 2868 samples\n",
      "Epoch 1/15\n",
      "11469/11469 [==============================] - 124s 11ms/step - loss: 1.5591 - accuracy: 0.2987 - val_loss: 1.4131 - val_accuracy: 0.4334\n",
      "Epoch 2/15\n",
      "11469/11469 [==============================] - 123s 11ms/step - loss: 1.4191 - accuracy: 0.4248 - val_loss: 1.3652 - val_accuracy: 0.4404\n",
      "Epoch 3/15\n",
      "11469/11469 [==============================] - 123s 11ms/step - loss: 1.3504 - accuracy: 0.4567 - val_loss: 1.3187 - val_accuracy: 0.4609\n",
      "Epoch 4/15\n",
      "11469/11469 [==============================] - 122s 11ms/step - loss: 1.3069 - accuracy: 0.4693 - val_loss: 1.2865 - val_accuracy: 0.4878\n",
      "Epoch 5/15\n",
      "11469/11469 [==============================] - 122s 11ms/step - loss: 1.2811 - accuracy: 0.4758 - val_loss: 1.2679 - val_accuracy: 0.4819\n",
      "Epoch 6/15\n",
      "11469/11469 [==============================] - 122s 11ms/step - loss: 1.2530 - accuracy: 0.4884 - val_loss: 1.2415 - val_accuracy: 0.5077\n",
      "Epoch 7/15\n",
      "11469/11469 [==============================] - 123s 11ms/step - loss: 1.2262 - accuracy: 0.5023 - val_loss: 1.1919 - val_accuracy: 0.5300\n",
      "Epoch 8/15\n",
      "11469/11469 [==============================] - 121s 11ms/step - loss: 1.2038 - accuracy: 0.5115 - val_loss: 1.2373 - val_accuracy: 0.5164\n",
      "Epoch 9/15\n",
      "11469/11469 [==============================] - 123s 11ms/step - loss: 1.1974 - accuracy: 0.5119 - val_loss: 1.2045 - val_accuracy: 0.5129\n",
      "Epoch 10/15\n",
      "11469/11469 [==============================] - 123s 11ms/step - loss: 1.1796 - accuracy: 0.5216 - val_loss: 1.2255 - val_accuracy: 0.5108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x196383b7648>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train, \n",
    "        epochs=15, \n",
    "        batch_size=64, \n",
    "        validation_data=(X_test, y_test), \n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.save_weights('model_1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(12157, 200)\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NUM_TOP_WORDS = None\n",
    "# tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "EMBED_SIZE = 200\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('./glove/glove.6B.200d.txt', encoding=\"utf8\")\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 868, 200)          2431400   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 2,552,305\n",
      "Trainable params: 2,552,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rnn2 = Sequential()\n",
    "rnn2.add(embedding_layer)\n",
    "rnn2.add(LSTM(300,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn2.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn2.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11469 samples, validate on 2868 samples\n",
      "Epoch 1/15\n",
      "11469/11469 [==============================] - 140s 12ms/step - loss: 1.2587 - accuracy: 0.4978 - val_loss: 1.0633 - val_accuracy: 0.5662\n",
      "Epoch 2/15\n",
      "11469/11469 [==============================] - 140s 12ms/step - loss: 1.0585 - accuracy: 0.5642 - val_loss: 1.0018 - val_accuracy: 0.6011\n",
      "Epoch 3/15\n",
      "11469/11469 [==============================] - 139s 12ms/step - loss: 1.0083 - accuracy: 0.5838 - val_loss: 0.9961 - val_accuracy: 0.6050\n",
      "Epoch 4/15\n",
      "11469/11469 [==============================] - 140s 12ms/step - loss: 0.9519 - accuracy: 0.6027 - val_loss: 0.9447 - val_accuracy: 0.6199\n",
      "Epoch 5/15\n",
      "11469/11469 [==============================] - 148s 13ms/step - loss: 0.9134 - accuracy: 0.6164 - val_loss: 0.9445 - val_accuracy: 0.6179\n",
      "Epoch 6/15\n",
      "11469/11469 [==============================] - 147s 13ms/step - loss: 0.8796 - accuracy: 0.6295 - val_loss: 0.9316 - val_accuracy: 0.6280\n",
      "Epoch 7/15\n",
      "11469/11469 [==============================] - 146s 13ms/step - loss: 0.8488 - accuracy: 0.6455 - val_loss: 0.9065 - val_accuracy: 0.6374\n",
      "Epoch 8/15\n",
      "11469/11469 [==============================] - 138s 12ms/step - loss: 0.9681 - accuracy: 0.6080 - val_loss: 0.9530 - val_accuracy: 0.6203\n",
      "Epoch 9/15\n",
      "11469/11469 [==============================] - 135s 12ms/step - loss: 0.8809 - accuracy: 0.6371 - val_loss: 0.9177 - val_accuracy: 0.6335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19632e187c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2.fit(X_train, \n",
    "        y_train, \n",
    "        epochs=15, \n",
    "        batch_size=64,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=2)]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2.save_weights('model_2_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
